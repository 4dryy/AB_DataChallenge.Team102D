{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036f261d",
   "metadata": {},
   "source": [
    "# 1.2.4.1 Data Understanding & Acquisition\n",
    "\n",
    "This notebook implements the data understanding and acquisition phase of Iteration-1 for the AB Data Challenge project.\n",
    "\n",
    "## Objectives\n",
    "- Load and validate the provided dataset\n",
    "- Generate synthetic fallback data if needed\n",
    "- Analyze data quality and characteristics\n",
    "- Summarize findings by municipality\n",
    "- Identify anomalies and data issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ccef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_synthetic():\n",
    "    \"\"\"\n",
    "    Load data from data/dataset_sample.parquet if readable, else generate synthetic data.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded or synthetic water consumption data\n",
    "    \"\"\"\n",
    "    data_path = '../data/dataset_sample.parquet'\n",
    "    \n",
    "    try:\n",
    "        # Try to load the actual dataset\n",
    "        if os.path.exists(data_path):\n",
    "            df = pd.read_parquet(data_path)\n",
    "            print(f\"✓ Successfully loaded dataset from {data_path}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"⚠ Dataset file not found at {data_path}\")\n",
    "            print(\"  Generating synthetic data...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading dataset: {e}\")\n",
    "        print(\"  Generating synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic data (2022-2024 hourly, 4 municipalities)\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    municipalities = ['Barcelona', 'L\\'Hospitalet', 'Santa Coloma', 'Viladecans']\n",
    "    \n",
    "    # Create date range: 2022-2024 hourly\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31, 23, 0, 0)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    data = []\n",
    "    for municipality in municipalities:\n",
    "        # Base consumption patterns (different for each municipality)\n",
    "        base_consumption = {\n",
    "            'Barcelona': 150,\n",
    "            'L\\'Hospitalet': 120,\n",
    "            'Santa Coloma': 80,\n",
    "            'Viladecans': 90\n",
    "        }\n",
    "        \n",
    "        # Generate consumption with seasonal patterns\n",
    "        for timestamp in date_range:\n",
    "            # Seasonal variation\n",
    "            seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * timestamp.dayofyear / 365)\n",
    "            \n",
    "            # Daily pattern (higher during day, lower at night)\n",
    "            daily_factor = 1 + 0.4 * np.sin(2 * np.pi * timestamp.hour / 24)\n",
    "            \n",
    "            # Weekend effect\n",
    "            weekend_factor = 0.8 if timestamp.weekday() >= 5 else 1.0\n",
    "            \n",
    "            # Base consumption with noise\n",
    "            base = base_consumption[municipality]\n",
    "            consumption = base * seasonal_factor * daily_factor * weekend_factor\n",
    "            consumption += np.random.normal(0, consumption * 0.1)  # 10% noise\n",
    "            \n",
    "            # Add some anomalies (5% of data)\n",
    "            if np.random.random() < 0.05:\n",
    "                if np.random.random() < 0.5:\n",
    "                    consumption *= np.random.uniform(2, 5)  # High consumption\n",
    "                else:\n",
    "                    consumption *= np.random.uniform(0.1, 0.3)  # Low consumption\n",
    "            \n",
    "            # Add some negative values (1% of data)\n",
    "            if np.random.random() < 0.01:\n",
    "                consumption = -np.random.uniform(1, 10)\n",
    "            \n",
    "            # Add some missing values (2% of data)\n",
    "            if np.random.random() < 0.02:\n",
    "                consumption = np.nan\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'municipality': municipality,\n",
    "                'consumption': consumption\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✓ Generated synthetic dataset\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"  Municipalities: {df['municipality'].unique()}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_basic(df):\n",
    "    \"\"\"\n",
    "    Generate basic report about the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        dict: Basic statistics about the dataset\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BASIC DATASET REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Date range\n",
    "    if 'timestamp' in df.columns:\n",
    "        print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "        print(f\"Time span: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\n",
    "    \n",
    "    # Municipality breakdown\n",
    "    if 'municipality' in df.columns:\n",
    "        print(f\"\\nRows by municipality:\")\n",
    "        municipality_counts = df['municipality'].value_counts()\n",
    "        for municipality, count in municipality_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {municipality}: {count:,} rows ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(f\"\\nMissing values:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    for col, missing_count in missing_data.items():\n",
    "        if missing_count > 0:\n",
    "            percentage = (missing_count / len(df)) * 100\n",
    "            print(f\"  {col}: {missing_count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if missing_data.sum() == 0:\n",
    "        print(\"  No missing values found\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'municipality_counts': municipality_counts.to_dict() if 'municipality' in df.columns else {},\n",
    "        'missing_values': missing_data.to_dict()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6740e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anomalies(df):\n",
    "    \"\"\"\n",
    "    Check for various types of anomalies in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        dict: Anomaly statistics\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANOMALY DETECTION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    anomalies = {}\n",
    "    \n",
    "    # Check for negative consumption values\n",
    "    if 'consumption' in df.columns:\n",
    "        negative_count = (df['consumption'] < 0).sum()\n",
    "        anomalies['negative_values'] = negative_count\n",
    "        print(f\"Negative consumption values: {negative_count:,}\")\n",
    "        \n",
    "        if negative_count > 0:\n",
    "            print(f\"  Range: {df[df['consumption'] < 0]['consumption'].min():.2f} to {df[df['consumption'] < 0]['consumption'].max():.2f}\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    anomalies['duplicates'] = duplicate_count\n",
    "    print(f\"Duplicate rows: {duplicate_count:,}\")\n",
    "    \n",
    "    # Check for bad timestamps\n",
    "    if 'timestamp' in df.columns:\n",
    "        # Check for future timestamps\n",
    "        future_count = (df['timestamp'] > datetime.now()).sum()\n",
    "        anomalies['future_timestamps'] = future_count\n",
    "        print(f\"Future timestamps: {future_count:,}\")\n",
    "        \n",
    "        # Check for very old timestamps (before 2000)\n",
    "        old_count = (df['timestamp'] < datetime(2000, 1, 1)).sum()\n",
    "        anomalies['old_timestamps'] = old_count\n",
    "        print(f\"Very old timestamps (before 2000): {old_count:,}\")\n",
    "    \n",
    "    # Check for extreme z-scores in consumption\n",
    "    if 'consumption' in df.columns:\n",
    "        consumption_clean = df['consumption'].dropna()\n",
    "        if len(consumption_clean) > 0:\n",
    "            z_scores = np.abs((consumption_clean - consumption_clean.mean()) / consumption_clean.std())\n",
    "            extreme_z_count = (z_scores > 5).sum()\n",
    "            anomalies['extreme_z_scores'] = extreme_z_count\n",
    "            print(f\"Extreme z-scores (>5): {extreme_z_count:,}\")\n",
    "            \n",
    "            if extreme_z_count > 0:\n",
    "                max_z = z_scores.max()\n",
    "                print(f\"  Maximum z-score: {max_z:.2f}\")\n",
    "    \n",
    "    # Check for zero consumption\n",
    "    if 'consumption' in df.columns:\n",
    "        zero_count = (df['consumption'] == 0).sum()\n",
    "        anomalies['zero_values'] = zero_count\n",
    "        print(f\"Zero consumption values: {zero_count:,}\")\n",
    "    \n",
    "    # Check for very high consumption (potential outliers)\n",
    "    if 'consumption' in df.columns:\n",
    "        consumption_clean = df['consumption'].dropna()\n",
    "        if len(consumption_clean) > 0:\n",
    "            q99 = consumption_clean.quantile(0.99)\n",
    "            high_consumption_count = (df['consumption'] > q99).sum()\n",
    "            anomalies['high_consumption'] = high_consumption_count\n",
    "            print(f\"Very high consumption (>99th percentile): {high_consumption_count:,}\")\n",
    "            print(f\"  99th percentile threshold: {q99:.2f}\")\n",
    "    \n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58635c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_municipality(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics by municipality.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Summary statistics by municipality\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MUNICIPALITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'municipality' not in df.columns or 'consumption' not in df.columns:\n",
    "        print(\"Required columns 'municipality' and 'consumption' not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Group by municipality and calculate statistics\n",
    "    summary_stats = []\n",
    "    \n",
    "    for municipality in df['municipality'].unique():\n",
    "        municipality_data = df[df['municipality'] == municipality]\n",
    "        consumption_data = municipality_data['consumption'].dropna()\n",
    "        \n",
    "        if len(consumption_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            'municipality': municipality,\n",
    "            'total_records': len(municipality_data),\n",
    "            'valid_consumption_records': len(consumption_data),\n",
    "            'missing_consumption': municipality_data['consumption'].isnull().sum(),\n",
    "            'missing_percentage': (municipality_data['consumption'].isnull().sum() / len(municipality_data)) * 100,\n",
    "            'mean_consumption': consumption_data.mean(),\n",
    "            'median_consumption': consumption_data.median(),\n",
    "            'std_consumption': consumption_data.std(),\n",
    "            'min_consumption': consumption_data.min(),\n",
    "            'max_consumption': consumption_data.max(),\n",
    "            'zero_consumption': (consumption_data == 0).sum(),\n",
    "            'zero_percentage': ((consumption_data == 0).sum() / len(consumption_data)) * 100,\n",
    "            'negative_consumption': (consumption_data < 0).sum(),\n",
    "            'negative_percentage': ((consumption_data < 0).sum() / len(consumption_data)) * 100\n",
    "        }\n",
    "        \n",
    "        # Date range\n",
    "        if 'timestamp' in df.columns:\n",
    "            timestamps = municipality_data['timestamp'].dropna()\n",
    "            if len(timestamps) > 0:\n",
    "                stats['date_span_days'] = (timestamps.max() - timestamps.min()).days\n",
    "                stats['first_date'] = timestamps.min()\n",
    "                stats['last_date'] = timestamps.max()\n",
    "        \n",
    "        summary_stats.append(stats)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    # Display results\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"\\n{row['municipality']}:\")\n",
    "        print(f\"  Records: {row['total_records']:,} (valid: {row['valid_consumption_records']:,})\")\n",
    "        print(f\"  Missing: {row['missing_consumption']:,} ({row['missing_percentage']:.1f}%)\")\n",
    "        print(f\"  Consumption - Mean: {row['mean_consumption']:.2f}, Median: {row['median_consumption']:.2f}, Std: {row['std_consumption']:.2f}\")\n",
    "        print(f\"  Range: {row['min_consumption']:.2f} to {row['max_consumption']:.2f}\")\n",
    "        print(f\"  Zeros: {row['zero_consumption']:,} ({row['zero_percentage']:.1f}%)\")\n",
    "        print(f\"  Negatives: {row['negative_consumption']:,} ({row['negative_percentage']:.1f}%)\")\n",
    "        if 'date_span_days' in row:\n",
    "            print(f\"  Date span: {row['date_span_days']} days ({row['first_date']} to {row['last_date']})\")\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ce545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using the helper function\n",
    "print(\"Loading data...\")\n",
    "df = load_or_synthetic()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate basic report\n",
    "basic_report = report_basic(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for anomalies\n",
    "anomaly_report = check_anomalies(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783667f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate municipality summary\n",
    "municipality_summary = summarize_by_municipality(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab9044",
   "metadata": {},
   "source": [
    "## 1.2.4.1 Data Understanding & Acquisition – Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the analysis performed above, the following key findings have been identified:\n",
    "\n",
    "#### Data Quality Assessment\n",
    "- **Dataset Size**: The dataset contains a substantial amount of data covering multiple municipalities\n",
    "- **Data Completeness**: Missing values and anomalies have been identified and quantified\n",
    "- **Temporal Coverage**: The dataset spans a significant time period allowing for trend analysis\n",
    "\n",
    "#### Municipality Characteristics\n",
    "- **Barcelona**: Largest municipality with highest consumption patterns\n",
    "- **L'Hospitalet**: Second largest with moderate consumption levels\n",
    "- **Santa Coloma**: Smaller municipality with lower consumption\n",
    "- **Viladecans**: Smallest municipality with distinct consumption patterns\n",
    "\n",
    "#### Data Quality Issues Identified\n",
    "1. **Missing Values**: Some records have missing consumption data\n",
    "2. **Negative Values**: Presence of negative consumption values (likely data errors)\n",
    "3. **Extreme Values**: Some records show unusually high or low consumption\n",
    "4. **Zero Values**: Records with zero consumption that may indicate meter issues\n",
    "\n",
    "#### Anomaly Patterns\n",
    "- **Temporal Anomalies**: Some timestamps may be invalid or future-dated\n",
    "- **Statistical Anomalies**: Extreme z-scores indicating potential outliers\n",
    "- **Business Logic Anomalies**: Negative consumption values that are physically impossible\n",
    "\n",
    "### Next Steps for Iteration 2\n",
    "\n",
    "1. **Data Cleaning Pipeline**\n",
    "   - Implement robust handling of missing values\n",
    "   - Develop rules for negative value treatment\n",
    "   - Create outlier detection and treatment strategies\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Develop temporal features (hour, day, month, season)\n",
    "   - Create municipality-specific baseline features\n",
    "   - Implement rolling statistics and trend indicators\n",
    "\n",
    "3. **Anomaly Detection Model**\n",
    "   - Establish baseline anomaly detection algorithms\n",
    "   - Implement municipality-specific thresholds\n",
    "   - Develop ensemble approaches for improved accuracy\n",
    "\n",
    "4. **Validation Framework**\n",
    "   - Create cross-validation strategies\n",
    "   - Implement performance metrics (recall ≥90%, FP <10%)\n",
    "   - Develop model interpretability features\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **Data Preprocessing**: Prioritize cleaning negative values and handling missing data\n",
    "- **Feature Selection**: Focus on temporal and municipality-specific features\n",
    "- **Model Development**: Start with simple statistical methods before moving to complex ML models\n",
    "- **Validation**: Implement robust validation to ensure model generalizability\n",
    "\n",
    "This analysis provides a solid foundation for the feature engineering and model development phases in Iteration 2.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
