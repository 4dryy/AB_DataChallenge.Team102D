{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf75d69",
   "metadata": {},
   "source": [
    "\n",
    "# Iteration 2 â€” 1.3.9 Feature Engineering Implementation\n",
    "**Team 102D Â· AB Data Challenge**\n",
    "\n",
    "This notebook covers:\n",
    "- **1.3.9** Feature Engineering Implementation  \n",
    "- **1.3.9.1** Definition & Computation of Candidate Features  \n",
    "- **1.3.9.2** Evaluation of Temporal & Behavioural Patterns  \n",
    "- **1.3.9.3** Documentation of Feature-Generation Process  \n",
    "\n",
    "**Focus:** anomaly detection using Telelectura data only (per reading with per-meter temporal context).  \n",
    "**Anomaly codes:** `32768` (faulty meter not yet replaced), `163840` (two consecutive 0-consumption periods).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a88fbf",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”§ Required installations (run once if needed)\n",
    "If you want to save outputs to Parquet, install one of these engines:\n",
    "```bash\n",
    "pip install -U pyarrow\n",
    "# or\n",
    "pip install -U fastparquet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb292176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you prefer to auto-install from the notebook, uncomment one of the lines below.\n",
    "# !pip install -U pyarrow\n",
    "# !pip install -U fastparquet\n",
    "# !pip install -U pandas numpy\n",
    "print(\"If Parquet save fails, install pyarrow or fastparquet (see the cell above).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup & paths ===\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set this to your repo root if running inside your project:\n",
    "# PROJECT_ROOT = r\"C:\\\\Users\\\\jofre\\\\Documents\\\\GitHub\\\\AB_DataChallenge.Team102D\"\n",
    "# PROJECT_ROOT = \"C:/Users/jofre/Documents/GitHub/AB_DataChallenge.Team102D\"\n",
    "PROJECT_ROOT = \".\"\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"iteration_2\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Expected cleaned data from 1.3.8\n",
    "CLEAN_CSV = os.path.join(RESULTS_DIR, \"cleaned_dataset_v2.csv\")\n",
    "DERIVED_DIR = os.path.join(DATA_DIR, \"derived\")\n",
    "FALLBACK_SAMPLE = os.path.join(DERIVED_DIR, \"dataset_sample.csv\")  # optional fallback\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    return None\n",
    "\n",
    "print(\"Looking for cleaned dataset:\", CLEAN_CSV)\n",
    "df = safe_read_csv(CLEAN_CSV)\n",
    "if df is None:\n",
    "    print(\"[warn] cleaned_dataset_v2.csv not found. Trying fallback:\", FALLBACK_SAMPLE)\n",
    "    df = safe_read_csv(FALLBACK_SAMPLE)\n",
    "\n",
    "if df is None:\n",
    "    raise SystemExit(\"No input data available. Run 1.3.8 first or place a CSV under results/iteration_2/cleaned_dataset_v2.csv\")\n",
    "\n",
    "print(\"Loaded df:\", df.shape)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed7db6",
   "metadata": {},
   "source": [
    "\n",
    "## Canonical Columns\n",
    "We standardize these names if present:\n",
    "- `polissa_id` (`POLIZA_SUMINISTRO`)\n",
    "- `num_serie_contador` (`NUMEROSERIECONTADOR`)\n",
    "- `consumption` (`CONSUMO_REAL`)\n",
    "- `datetime` (`FECHA_HORA` â†’ parsed to datetime)\n",
    "- `codi_anomalia`\n",
    "- `data_inici`, `data_fi` (period boundaries if applicable)\n",
    "- `date`, `year`, `month`, `dayofweek`, `hour` (derived)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply canonical mapping (adapted to your columns)\n",
    "CANONICAL_MAP = {\n",
    "    \"POLIZA_SUMINISTRO\": \"polissa_id\",\n",
    "    \"NUMEROSERIECONTADOR\": \"num_serie_contador\",\n",
    "    \"CONSUMO_REAL\": \"consumption\",\n",
    "    \"FECHA_HORA\": \"datetime\",\n",
    "    \"CODI_ANOMALIA\": \"codi_anomalia\",\n",
    "    \"DATA_INICI\": \"data_inici\",\n",
    "    \"DATA_FI\": \"data_fi\",\n",
    "    \"DATE\": \"date\",\n",
    "    \"YEAR\": \"year\",\n",
    "}\n",
    "\n",
    "# Normalize and map\n",
    "new_cols = []\n",
    "for c in df.columns:\n",
    "    key = c.strip().upper()\n",
    "    new_cols.append(CANONICAL_MAP.get(key, c))\n",
    "df.columns = new_cols\n",
    "\n",
    "# Parse time fields\n",
    "if \"datetime\" in df.columns:\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "if \"date\" in df.columns:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "if \"data_inici\" in df.columns:\n",
    "    df[\"data_inici\"] = pd.to_datetime(df[\"data_inici\"], errors=\"coerce\")\n",
    "if \"data_fi\" in df.columns:\n",
    "    df[\"data_fi\"] = pd.to_datetime(df[\"data_fi\"], errors=\"coerce\")\n",
    "\n",
    "# Temporal context\n",
    "if \"datetime\" in df.columns:\n",
    "    df[\"year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "elif \"date\" in df.columns:\n",
    "    dt = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"year\"] = dt.dt.year\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    df[\"dayofweek\"] = dt.dt.dayofweek\n",
    "\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a28a95",
   "metadata": {},
   "source": [
    "\n",
    "## Labels & Flags\n",
    "- **Label** `y_anom = 1` if `codi_anomalia âˆˆ {32768, 163840}`, else 0  \n",
    "- **Flags**: zero/negative consumption (non-destructive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a998ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_labels_and_flags(dfin: pd.DataFrame) -> pd.DataFrame:\n",
    "    import numpy as np\n",
    "    dfout = dfin.copy()\n",
    "    # Label\n",
    "    if \"codi_anomalia\" in dfout.columns:\n",
    "        dfout[\"y_anom\"] = dfout[\"codi_anomalia\"].isin([32768, 163840]).astype(int)\n",
    "    else:\n",
    "        dfout[\"y_anom\"] = np.nan  # label unavailable\n",
    "    \n",
    "    # Flags\n",
    "    if \"consumption\" in dfout.columns:\n",
    "        dfout[\"flag_zero_consumption\"] = (pd.to_numeric(dfout[\"consumption\"], errors=\"coerce\") == 0).astype(float)\n",
    "        dfout[\"flag_negative_consumption\"] = (pd.to_numeric(dfout[\"consumption\"], errors=\"coerce\") < 0).astype(float)\n",
    "    else:\n",
    "        dfout[\"flag_zero_consumption\"] = np.nan\n",
    "        dfout[\"flag_negative_consumption\"] = np.nan\n",
    "    return dfout\n",
    "\n",
    "df = make_labels_and_flags(df)\n",
    "display(df[[\"codi_anomalia\",\"y_anom\",\"flag_zero_consumption\",\"flag_negative_consumption\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ba23c",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Set (per reading, per meter context)\n",
    "We compute:\n",
    "- **Lags & deltas**: `cons_lag1`, `cons_lag2`, `delta1`, `delta2`\n",
    "- **Rolling stats** on last N readings: mean/std/min/max and zero/neg ratios (N âˆˆ {3, 12, 24})\n",
    "- **Meter-level z-score** vs. historical mean/std\n",
    "- **Period duration** in hours (if `data_inici` & `data_fi` exist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701edf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROLL_WINDOWS = [3, 12, 24]  # last-N readings\n",
    "\n",
    "def _group_sort(dfin: pd.DataFrame):\n",
    "    key = \"num_serie_contador\" if \"num_serie_contador\" in dfin.columns else (\n",
    "          \"polissa_id\" if \"polissa_id\" in dfin.columns else None)\n",
    "    if key is None:\n",
    "        raise ValueError(\"No meter/policy identifier found. Expect one of ['num_serie_contador','polissa_id'].\")\n",
    "    if \"datetime\" in dfin.columns:\n",
    "        dfin = dfin.sort_values([key, \"datetime\"])\n",
    "    elif \"date\" in dfin.columns:\n",
    "        dfin = dfin.sort_values([key, \"date\"])\n",
    "    else:\n",
    "        dfin = dfin.sort_values([key]).copy()\n",
    "    return dfin, key\n",
    "\n",
    "def make_features(dfin: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfout = dfin.copy()\n",
    "    dfout, key = _group_sort(dfout)\n",
    "\n",
    "    if \"consumption\" not in dfout.columns:\n",
    "        raise ValueError(\"Missing 'consumption'. Map CONSUMO_REAL -> consumption first.\")\n",
    "    dfout[\"consumption\"] = pd.to_numeric(dfout[\"consumption\"], errors=\"coerce\")\n",
    "\n",
    "    # Lags/Deltas\n",
    "    dfout[\"cons_lag1\"] = dfout.groupby(key)[\"consumption\"].shift(1)\n",
    "    dfout[\"cons_lag2\"] = dfout.groupby(key)[\"consumption\"].shift(2)\n",
    "    dfout[\"delta1\"] = dfout[\"consumption\"] - dfout[\"cons_lag1\"]\n",
    "    dfout[\"delta2\"] = dfout[\"consumption\"] - dfout[\"cons_lag2\"]\n",
    "\n",
    "    # Rolling stats\n",
    "    for N in ROLL_WINDOWS:\n",
    "        grp = dfout.groupby(key)[\"consumption\"]\n",
    "        dfout[f\"roll{N}_mean\"] = grp.shift(1).rolling(N, min_periods=1).mean()\n",
    "        dfout[f\"roll{N}_std\"]  = grp.shift(1).rolling(N, min_periods=2).std()\n",
    "        dfout[f\"roll{N}_min\"]  = grp.shift(1).rolling(N, min_periods=1).min()\n",
    "        dfout[f\"roll{N}_max\"]  = grp.shift(1).rolling(N, min_periods=1).max()\n",
    "\n",
    "        zeros = (grp.shift(1) == 0).astype(float)\n",
    "        negs  = (grp.shift(1) < 0).astype(float)\n",
    "        dfout[f\"roll{N}_zero_ratio\"] = zeros.groupby(dfout[key]).rolling(N, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        dfout[f\"roll{N}_neg_ratio\"]  = negs.groupby(dfout[key]).rolling(N, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    # Meter-level normalization\n",
    "    meter_stats = dfout.groupby(key)[\"consumption\"].agg(meter_mean=\"mean\", meter_std=\"std\")\n",
    "    dfout = dfout.merge(meter_stats, left_on=key, right_index=True, how=\"left\")\n",
    "    dfout[\"cons_z_meter\"] = (dfout[\"consumption\"] - dfout[\"meter_mean\"]) / dfout[\"meter_std\"].replace(0, np.nan)\n",
    "\n",
    "    # Period duration\n",
    "    if {\"data_inici\",\"data_fi\"}.issubset(dfout.columns):\n",
    "        di = pd.to_datetime(dfout[\"data_inici\"], errors=\"coerce\")\n",
    "        df_ = pd.to_datetime(dfout[\"data_fi\"], errors=\"coerce\")\n",
    "        dfout[\"period_hours\"] = (df_ - di).dt.total_seconds() / 3600.0\n",
    "\n",
    "    return dfout\n",
    "\n",
    "feat_df = make_features(df)\n",
    "display(feat_df.head(5))\n",
    "print(\"Feature shape:\", feat_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb0139",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3.9.2 â€” Quick Temporal/Behavioural Checks\n",
    "Numeric summaries and a **robust** correlation with the label (skips low-N & zero-variance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summaries = {}\n",
    "\n",
    "# Robust correlation with label\n",
    "label_col = \"y_anom\"\n",
    "min_n = 50\n",
    "\n",
    "def safe_corr(x: pd.Series, y: pd.Series, min_n: int = 50):\n",
    "    s = pd.concat([x, y], axis=1).dropna()\n",
    "    if len(s) < min_n:\n",
    "        return np.nan\n",
    "    if s.iloc[:, 0].std(ddof=1) == 0 or s.iloc[:, 1].std(ddof=1) == 0:\n",
    "        return np.nan\n",
    "    return s.iloc[:, 0].corr(s.iloc[:, 1])\n",
    "\n",
    "corr_with_y = {}\n",
    "if label_col in feat_df.columns:\n",
    "    num_cols = [c for c in feat_df.columns if pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "    for c in num_cols:\n",
    "        if c == label_col:\n",
    "            continue\n",
    "        r = safe_corr(feat_df[c], feat_df[label_col], min_n=min_n)\n",
    "        if not np.isnan(r):\n",
    "            corr_with_y[c] = float(r)\n",
    "\n",
    "if corr_with_y:\n",
    "    corr_series = pd.Series(corr_with_y).sort_values(ascending=False)\n",
    "    corr_series.head(25).to_csv(os.path.join(RESULTS_DIR, \"fe_corr_with_label_top25.csv\"))\n",
    "    summaries[\"corr_top25_path\"] = \"fe_corr_with_label_top25.csv\"\n",
    "    print(f\"[ok] Saved {len(corr_with_y)} valid correlations.\")\n",
    "else:\n",
    "    print(\"[info] No valid correlations (likely zero variance or insufficient pairs).\")\n",
    "\n",
    "# Numeric describe\n",
    "num_desc = feat_df.describe(include=\"number\").T\n",
    "num_desc.to_csv(os.path.join(RESULTS_DIR, \"fe_numeric_describe.csv\"))\n",
    "summaries[\"numeric_describe_path\"] = \"fe_numeric_describe.csv\"\n",
    "\n",
    "# Year coverage\n",
    "if \"year\" in feat_df.columns:\n",
    "    cov_year = feat_df.groupby(\"year\").size().reset_index(name=\"n_records\")\n",
    "    cov_year.to_csv(os.path.join(RESULTS_DIR, \"fe_coverage_year.csv\"), index=False)\n",
    "    summaries[\"coverage_year_path\"] = \"fe_coverage_year.csv\"\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"fe_quick_summaries.json\"), \"w\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(\"Wrote summaries:\", summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a6c5b",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3.9.3 â€” Save Outputs & Document Features\n",
    "CSV is always saved. Parquet is attempted (requires pyarrow/fastparquet). A feature dictionary is generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save features (CSV)\n",
    "fe_csv = os.path.join(RESULTS_DIR, \"features_v2.csv\")\n",
    "feat_df.to_csv(fe_csv, index=False)\n",
    "print(\"[saved]\", fe_csv)\n",
    "\n",
    "# Safe Parquet write\n",
    "try:\n",
    "    import pandas as pd\n",
    "    fe_parquet = os.path.join(RESULTS_DIR, \"features_v2.parquet\")\n",
    "    feat_df.to_parquet(fe_parquet, index=False)\n",
    "    print(\"[saved]\", fe_parquet)\n",
    "except Exception as e:\n",
    "    print(\"[warn] Could not write parquet:\", e)\n",
    "\n",
    "# Feature dictionary\n",
    "feature_dict = []\n",
    "\n",
    "def add_desc(name, desc, dtype=\"numeric\", grain=\"reading\"):\n",
    "    feature_dict.append({\"name\": name, \"description\": desc, \"dtype\": dtype, \"grain\": grain})\n",
    "\n",
    "core_fields = [\"polissa_id\",\"num_serie_contador\",\"datetime\",\"date\",\"year\",\"month\",\"dayofweek\",\"hour\",\n",
    "               \"codi_anomalia\",\"data_inici\",\"data_fi\",\"period_hours\",\"y_anom\",\"consumption\",\n",
    "               \"flag_zero_consumption\",\"flag_negative_consumption\",\n",
    "               \"cons_lag1\",\"cons_lag2\",\"delta1\",\"delta2\",\"meter_mean\",\"meter_std\",\"cons_z_meter\"]\n",
    "for base in core_fields:\n",
    "    if base in feat_df.columns:\n",
    "        add_desc(base, f\"Core/derived field: {base}\")\n",
    "\n",
    "for N in [3,12,24]:\n",
    "    for stat in [\"mean\",\"std\",\"min\",\"max\",\"zero_ratio\",\"neg_ratio\"]:\n",
    "        col = f\"roll{N}_{stat}\"\n",
    "        if col in feat_df.columns:\n",
    "            add_desc(col, f\"Rolling {stat} over last {N} readings (excluding current).\")\n",
    "\n",
    "dict_path = os.path.join(RESULTS_DIR, \"feature_dictionary_v2.json\")\n",
    "with open(dict_path, \"w\") as f:\n",
    "    json.dump(feature_dict, f, indent=2, ensure_ascii=False)\n",
    "print(\"[saved]\", dict_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054d1f6",
   "metadata": {},
   "source": [
    "\n",
    "# Visual Analysis (Optional but Recommended)\n",
    "Simple plots to aid **1.3.9.2 Evaluation** and reporting.  \n",
    "Each chart uses Matplotlib (no seaborn) and appears in its own figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee214755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"consumption\" in feat_df.columns:\n",
    "    plt.figure()\n",
    "    feat_df[\"consumption\"].dropna().hist(bins=50)\n",
    "    plt.title(\"Distribution of Consumption\")\n",
    "    plt.xlabel(\"Consumption\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[info] 'consumption' column not found; skipping histogram.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0004d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"year\" in feat_df.columns:\n",
    "    counts = feat_df[\"year\"].value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Records per Year\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Number of Records\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[info] 'year' column not found; skipping records-per-year plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_col = \"num_serie_contador\" if \"num_serie_contador\" in feat_df.columns else (\"polissa_id\" if \"polissa_id\" in feat_df.columns else None)\n",
    "if id_col is not None and \"datetime\" in feat_df.columns and \"roll12_mean\" in feat_df.columns:\n",
    "    meter_id = feat_df[id_col].dropna().unique()\n",
    "    if len(meter_id) > 0:\n",
    "        meter_id = meter_id[0]\n",
    "        sub = feat_df[feat_df[id_col] == meter_id].copy()\n",
    "        sub = sub.sort_values(\"datetime\")\n",
    "        plt.figure()\n",
    "        sub.set_index(\"datetime\")[[\"consumption\"]].plot(legend=True)\n",
    "        plt.title(f\"Consumption over time â€” {id_col}={meter_id}\")\n",
    "        plt.xlabel(\"Datetime\")\n",
    "        plt.ylabel(\"Consumption\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        sub.set_index(\"datetime\")[[\"roll12_mean\"]].plot(legend=True)\n",
    "        plt.title(f\"Roll12 Mean over time â€” {id_col}={meter_id}\")\n",
    "        plt.xlabel(\"Datetime\")\n",
    "        plt.ylabel(\"Roll12 Mean\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[info] No meter ids available for time series plot.\")\n",
    "else:\n",
    "    print(\"[info] Missing id/time/roll12_mean for time series plots; skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80905d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "na_ratios = feat_df.isna().mean().sort_values(ascending=False).head(20)\n",
    "plt.figure()\n",
    "na_ratios.plot(kind=\"bar\")\n",
    "plt.title(\"Top-20 Features by Missingness Ratio\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"NA Ratio\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"y_anom\" in feat_df.columns:\n",
    "    vc = feat_df[\"y_anom\"].value_counts(dropna=False).sort_index()\n",
    "    plt.figure()\n",
    "    vc.plot(kind=\"bar\")\n",
    "    plt.title(\"Label Distribution: y_anom\")\n",
    "    plt.xlabel(\"y_anom\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[info] 'y_anom' not found; skipping label distribution plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ca3d6",
   "metadata": {},
   "source": [
    "\n",
    "## Optional Diagnostics\n",
    "Helps explain missing correlations (constant labels / zero-variance features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label sanity\n",
    "if \"y_anom\" in feat_df.columns:\n",
    "    print(\"y_anom counts:\", feat_df[\"y_anom\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Zero-variance numeric features\n",
    "num_cols = [c for c in feat_df.columns if pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "zero_var = [c for c in num_cols if feat_df[c].std(skipna=True) == 0]\n",
    "print(f\"Zero-variance numeric features: {len(zero_var)}\")\n",
    "if zero_var:\n",
    "    print(zero_var[:20], \"...\")\n",
    "\n",
    "# NA burden across numeric features (top 15)\n",
    "na_burden = (feat_df[num_cols].isna().mean().sort_values(ascending=False)).head(15)\n",
    "print(\"Top-15 NA ratios among numeric features:\")\n",
    "display(na_burden)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
