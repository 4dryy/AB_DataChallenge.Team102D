{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c43e538",
   "metadata": {},
   "source": [
    "\n",
    "# Iteration 2 — 1.3.8 Data Analysis on AB Dataset\n",
    "**Team 102D · AB Data Challenge**\n",
    "\n",
    "This notebook implements Task **1.3.8** in three sub‑steps:\n",
    "- **1.3.8.1 Data Quality Re‑check**\n",
    "- **1.3.8.2 Exploratory Analysis on New/Updated Variables**\n",
    "- **1.3.8.3 Adjustment of Cleaning Rules if Needed**\n",
    "\n",
    "> Notes\n",
    "> - We start by converting **Parquet → CSV** to quickly inspect data (requested by the team).\n",
    "> - Keep the code **simple and well‑commented**. We only adjust cleaning if necessary.\n",
    "> - Outputs are written to `results/iteration_2/` for traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c67f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup & paths ===\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Root of your repo (adjust if running directly inside the project)\n",
    "PROJECT_ROOT = r\"C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\"  # set to your repo root when moving this notebook\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")  # expects parquet files here\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"iteration_2\")\n",
    "DERIVED_DIR = os.path.join(DATA_DIR, \"derived\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(DERIVED_DIR, exist_ok=True)\n",
    "\n",
    "# Helper: safe write\n",
    "def _save(df, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[saved] {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f2665",
   "metadata": {},
   "source": [
    "\n",
    "## Step 0 — Quick Conversion: Parquet → CSV\n",
    "We convert available Parquet files under `data/` to CSV for immediate inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd76c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Parquet files: ['C:\\\\Users\\\\joan\\\\Desktop\\\\FEINA\\\\UPF\\\\Course\\\\Fourth_year\\\\Primer_Trimestre\\\\Project_Management\\\\AB\\\\AB_DataChallenge.Team102D\\\\data\\\\big_dataset.parquet', 'C:\\\\Users\\\\joan\\\\Desktop\\\\FEINA\\\\UPF\\\\Course\\\\Fourth_year\\\\Primer_Trimestre\\\\Project_Management\\\\AB\\\\AB_DataChallenge.Team102D\\\\data\\\\dataset_sample.parquet']\n",
      "[ok] Parquet→CSV: C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\data\\big_dataset.parquet -> C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\data\\derived\\big_dataset.csv\n",
      "[ok] Parquet→CSV: C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\data\\dataset_sample.parquet -> C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\data\\derived\\dataset_sample.csv\n",
      "Using WORK_CSV: C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\data\\derived\\big_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find Parquet files inside data/\n",
    "parquet_files = glob.glob(os.path.join(DATA_DIR, \"*.parquet\"))\n",
    "print(\"Found Parquet files:\", parquet_files)\n",
    "\n",
    "converted_csv_paths = []\n",
    "for pq in parquet_files:\n",
    "    try:\n",
    "        df_tmp = pd.read_parquet(pq)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not read {pq}: {e}\")\n",
    "        continue\n",
    "    base = os.path.splitext(os.path.basename(pq))[0]\n",
    "    out_csv = os.path.join(DERIVED_DIR, f\"{base}.csv\")\n",
    "    df_tmp.to_csv(out_csv, index=False)\n",
    "    converted_csv_paths.append(out_csv)\n",
    "    print(f\"[ok] Parquet→CSV: {pq} -> {out_csv}\")\n",
    "\n",
    "# Pick the first CSV as our working file (adjust if you have multiple tables)\n",
    "if converted_csv_paths:\n",
    "    WORK_CSV = converted_csv_paths[0]\n",
    "    print(\"Using WORK_CSV:\", WORK_CSV)\n",
    "else:\n",
    "    # If you have multiple input tables (telelectura, anomalies, billing, etc.), \n",
    "    # add them below once available (placeholders for now).\n",
    "    WORK_CSV = None\n",
    "    print(\"[info] No CSV produced yet. Place your parquet files in `data/`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e90ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files loaded\n",
      "Sample shape: (25977, 7)\n",
      "Big dataset shape: (25977, 7)\n",
      "\n",
      "Columns only in sample: set()\n",
      "Columns only in big: set()\n",
      "\n",
      "Columns with different dtypes: {}\n",
      "\n",
      "Head of sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLIZA_SUMINISTRO</th>\n",
       "      <th>NUMEROSERIECONTADOR</th>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <th>FECHA_HORA</th>\n",
       "      <th>CODI_ANOMALIA</th>\n",
       "      <th>DATA_INICI</th>\n",
       "      <th>DATA_FI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-17 00:00:00</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2024-06-17 00:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2024-06-17 01:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POLIZA_SUMINISTRO NUMEROSERIECONTADOR  CONSUMO_REAL           FECHA_HORA  \\\n",
       "0  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL           NaN  2024-06-17 00:00:00   \n",
       "1  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL          21.0  2024-06-17 00:56:13   \n",
       "2  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL           7.0  2024-06-17 01:56:13   \n",
       "\n",
       "   CODI_ANOMALIA  DATA_INICI     DATA_FI  \n",
       "0         163840  2024-07-15  2024-09-13  \n",
       "1         163840  2024-07-15  2024-09-13  \n",
       "2         163840  2024-07-15  2024-09-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head of big:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLIZA_SUMINISTRO</th>\n",
       "      <th>NUMEROSERIECONTADOR</th>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <th>FECHA_HORA</th>\n",
       "      <th>CODI_ANOMALIA</th>\n",
       "      <th>DATA_INICI</th>\n",
       "      <th>DATA_FI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-17 00:00:00</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2024-06-17 00:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2024-06-17 01:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POLIZA_SUMINISTRO NUMEROSERIECONTADOR  CONSUMO_REAL           FECHA_HORA  \\\n",
       "0  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL           NaN  2024-06-17 00:00:00   \n",
       "1  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL          21.0  2024-06-17 00:56:13   \n",
       "2  JSM5YS4KVQUI5DQA    RMQO6U3MP5TS4QUL           7.0  2024-06-17 01:56:13   \n",
       "\n",
       "   CODI_ANOMALIA  DATA_INICI     DATA_FI  \n",
       "0         163840  2024-07-15  2024-09-13  \n",
       "1         163840  2024-07-15  2024-09-13  \n",
       "2         163840  2024-07-15  2024-09-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_sample</th>\n",
       "      <th>mean_sample</th>\n",
       "      <th>std_sample</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>25%_sample</th>\n",
       "      <th>50%_sample</th>\n",
       "      <th>75%_sample</th>\n",
       "      <th>max_sample</th>\n",
       "      <th>count_big</th>\n",
       "      <th>mean_big</th>\n",
       "      <th>std_big</th>\n",
       "      <th>min_big</th>\n",
       "      <th>25%_big</th>\n",
       "      <th>50%_big</th>\n",
       "      <th>75%_big</th>\n",
       "      <th>max_big</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CODI_ANOMALIA</th>\n",
       "      <td>25977.0</td>\n",
       "      <td>115330.695307</td>\n",
       "      <td>63286.753163</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>25977.0</td>\n",
       "      <td>115330.695307</td>\n",
       "      <td>63286.753163</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>163840.0</td>\n",
       "      <td>163840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONSUMO_REAL</th>\n",
       "      <td>21528.0</td>\n",
       "      <td>2.795940</td>\n",
       "      <td>10.903834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>21528.0</td>\n",
       "      <td>2.795940</td>\n",
       "      <td>10.903834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count_sample    mean_sample    std_sample  min_sample  \\\n",
       "CODI_ANOMALIA       25977.0  115330.695307  63286.753163     32768.0   \n",
       "CONSUMO_REAL        21528.0       2.795940     10.903834         0.0   \n",
       "\n",
       "               25%_sample  50%_sample  75%_sample  max_sample  count_big  \\\n",
       "CODI_ANOMALIA     32768.0    163840.0    163840.0    163840.0    25977.0   \n",
       "CONSUMO_REAL          0.0         0.0         0.0       419.0    21528.0   \n",
       "\n",
       "                    mean_big       std_big  min_big  25%_big   50%_big  \\\n",
       "CODI_ANOMALIA  115330.695307  63286.753163  32768.0  32768.0  163840.0   \n",
       "CONSUMO_REAL        2.795940     10.903834      0.0      0.0       0.0   \n",
       "\n",
       "                75%_big   max_big  \n",
       "CODI_ANOMALIA  163840.0  163840.0  \n",
       "CONSUMO_REAL        0.0     419.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 IDs overlap out of 10 in sample and 10 in big.\n"
     ]
    }
   ],
   "source": [
    "# === Compare dataset_sample.csv vs big_dataset.csv ===\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths (adapt to your structure)\n",
    "sample_path = os.path.join(DERIVED_DIR, \"dataset_sample.csv\")\n",
    "big_path = os.path.join(DERIVED_DIR, \"big_dataset.csv\")\n",
    "\n",
    "# Load both (use low_memory=False to avoid dtype warnings)\n",
    "df_sample = pd.read_csv(sample_path, low_memory=False)\n",
    "df_big = pd.read_csv(big_path, low_memory=False)\n",
    "\n",
    "print(\"✅ Files loaded\")\n",
    "print(f\"Sample shape: {df_sample.shape}\")\n",
    "print(f\"Big dataset shape: {df_big.shape}\\n\")\n",
    "\n",
    "# --- 1️⃣ Column comparison ---\n",
    "sample_cols = set(df_sample.columns)\n",
    "big_cols = set(df_big.columns)\n",
    "print(\"Columns only in sample:\", sample_cols - big_cols)\n",
    "print(\"Columns only in big:\", big_cols - sample_cols)\n",
    "\n",
    "# --- 2️⃣ Data type comparison (for shared columns) ---\n",
    "shared_cols = list(sample_cols & big_cols)\n",
    "dtype_diff = {\n",
    "    c: (df_sample[c].dtype, df_big[c].dtype)\n",
    "    for c in shared_cols\n",
    "    if df_sample[c].dtype != df_big[c].dtype\n",
    "}\n",
    "print(\"\\nColumns with different dtypes:\", dtype_diff)\n",
    "\n",
    "# --- 3️⃣ Basic row-level diagnostics ---\n",
    "print(\"\\nHead of sample:\")\n",
    "display(df_sample.head(3))\n",
    "print(\"\\nHead of big:\")\n",
    "display(df_big.head(3))\n",
    "\n",
    "# --- 4️⃣ Basic numeric summary comparison ---\n",
    "num_cols = [c for c in shared_cols if pd.api.types.is_numeric_dtype(df_big[c])]\n",
    "summary_sample = df_sample[num_cols].describe().T\n",
    "summary_big = df_big[num_cols].describe().T\n",
    "diff_summary = summary_sample.join(summary_big, lsuffix=\"_sample\", rsuffix=\"_big\", how=\"outer\")\n",
    "display(diff_summary)\n",
    "\n",
    "# --- 5️⃣ Optional: check overlap if unique identifier exists ---\n",
    "id_candidates = [c for c in df_big.columns if \"POLIZA\" in c.upper() or \"ID\" in c.upper()]\n",
    "if id_candidates:\n",
    "    id_col = id_candidates[0]\n",
    "    overlap = len(set(df_sample[id_col]) & set(df_big[id_col]))\n",
    "    print(f\"\\n{overlap} IDs overlap out of {df_sample[id_col].nunique()} in sample and {df_big[id_col].nunique()} in big.\")\n",
    "else:\n",
    "    print(\"\\nNo clear ID column found for overlap check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85ab15",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 — Load & Light Schema Mapping\n",
    "We define a minimal **canonical schema** to standardize column names across sources.  \n",
    "Only map columns that exist; everything else remains untouched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1df22a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25977, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polissa_id</th>\n",
       "      <th>num_serie_contador</th>\n",
       "      <th>consumption</th>\n",
       "      <th>FECHA_HORA</th>\n",
       "      <th>codi_anomalia</th>\n",
       "      <th>data_inici</th>\n",
       "      <th>data_fi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-17 00:00:00</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2024-06-17 00:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JSM5YS4KVQUI5DQA</td>\n",
       "      <td>RMQO6U3MP5TS4QUL</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2024-06-17 01:56:13</td>\n",
       "      <td>163840</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>2024-09-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polissa_id num_serie_contador  consumption           FECHA_HORA  \\\n",
       "0  JSM5YS4KVQUI5DQA   RMQO6U3MP5TS4QUL          NaN  2024-06-17 00:00:00   \n",
       "1  JSM5YS4KVQUI5DQA   RMQO6U3MP5TS4QUL         21.0  2024-06-17 00:56:13   \n",
       "2  JSM5YS4KVQUI5DQA   RMQO6U3MP5TS4QUL          7.0  2024-06-17 01:56:13   \n",
       "\n",
       "   codi_anomalia data_inici    data_fi  \n",
       "0         163840 2024-07-15 2024-09-13  \n",
       "1         163840 2024-07-15 2024-09-13  \n",
       "2         163840 2024-07-15 2024-09-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# === Canonical mapping adapted to your dataset ===\n",
    "CANONICAL_MAP = {\n",
    "    # --- time fields ---\n",
    "    \"DATETIME\": \"datetime\",\n",
    "    \"DATE\": \"date\",\n",
    "    \"DATA_INICI\": \"data_inici\",   # start of consumption period\n",
    "    \"DATA_FI\": \"data_fi\",         # end of consumption period\n",
    "    \"YEAR\": \"year\",\n",
    "\n",
    "    # --- identifiers ---\n",
    "    \"POLIZA_SUMINISTRO\": \"polissa_id\",\n",
    "    \"NUMEROSERIECONTADOR\": \"num_serie_contador\",\n",
    "\n",
    "    # --- main variable ---\n",
    "    \"CONSUMO_REAL\": \"consumption\",\n",
    "\n",
    "    # --- anomaly info ---\n",
    "    \"CODI_ANOMALIA\": \"codi_anomalia\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_csv_mapped(path):\n",
    "    \"\"\"\n",
    "    Loads the CSV converted from Parquet and applies canonical renaming\n",
    "    based on Iteration 2 column structure.\n",
    "    \"\"\"\n",
    "    if path is None or not os.path.exists(path):\n",
    "        raise FileNotFoundError(\"WORK_CSV not set or file does not exist. Ensure Parquet→CSV ran and a file is available.\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize column names: strip spaces, uppercase, map through CANONICAL_MAP\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        key = c.strip().upper()\n",
    "        new_cols.append(CANONICAL_MAP.get(key, c))\n",
    "    df.columns = new_cols\n",
    "\n",
    "    # Parse datetime and date fields safely\n",
    "    if \"datetime\" in df.columns:\n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "    if \"data_inici\" in df.columns:\n",
    "        df[\"data_inici\"] = pd.to_datetime(df[\"data_inici\"], errors=\"coerce\")\n",
    "    if \"data_fi\" in df.columns:\n",
    "        df[\"data_fi\"] = pd.to_datetime(df[\"data_fi\"], errors=\"coerce\")\n",
    "\n",
    "    # Derive year if not already present\n",
    "    if \"year\" not in df.columns and \"date\" in df.columns:\n",
    "        df[\"year\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_csv_mapped(WORK_CSV) if WORK_CSV else None\n",
    "if df is not None:\n",
    "    print(df.shape)\n",
    "    display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d4c1f",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3.8.1 — Data Quality Re‑check\n",
    "We verify coherence across **periods (years)** and **municipalities**, and produce a short data quality report.\n",
    "- Missingness per column\n",
    "- Duplicates\n",
    "- Basic range sanity for `consumption` if present\n",
    "- Coverage table by `municipi × year`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737dc143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\results\\iteration_2\\qcheck_missingness.csv\n",
      "Duplicate rows: 0\n",
      "[saved] C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\results\\iteration_2\\qcheck_anomaly_codes.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Guard: ensure data loaded ---\n",
    "if df is None:\n",
    "    raise SystemExit(\"No data loaded. Make sure a Parquet exists under `data/` and was converted to CSV.\")\n",
    "\n",
    "# --- Missingness summary ---\n",
    "na_summary = df.isna().mean().reset_index()\n",
    "na_summary.columns = [\"column\", \"na_ratio\"]\n",
    "_save(na_summary, os.path.join(RESULTS_DIR, \"qcheck_missingness.csv\"))\n",
    "\n",
    "# --- Duplicate rows ---\n",
    "dup_count = df.duplicated().sum()\n",
    "with open(os.path.join(RESULTS_DIR, \"qcheck_duplicates.json\"), \"w\") as f:\n",
    "    json.dump({\"duplicate_rows\": int(dup_count)}, f)\n",
    "print(f\"Duplicate rows: {dup_count}\")\n",
    "\n",
    "# --- Consumption sanity (if present) ---\n",
    "range_report = {}\n",
    "if \"consumption\" in df.columns:\n",
    "    c = df[\"consumption\"]\n",
    "    range_report = {\n",
    "        \"min\": float(c.min(skipna=True)),\n",
    "        \"p01\": float(c.quantile(0.01)),\n",
    "        \"median\": float(c.median(skipna=True)),\n",
    "        \"p99\": float(c.quantile(0.99)),\n",
    "        \"max\": float(c.max(skipna=True)),\n",
    "        \"negatives_count\": int((c < 0).sum()),\n",
    "        \"zeros_count\": int((c == 0).sum()),\n",
    "    }\n",
    "with open(os.path.join(RESULTS_DIR, \"qcheck_consumption_range.json\"), \"w\") as f:\n",
    "    json.dump(range_report, f, indent=2)\n",
    "\n",
    "# --- Coverage by year (simple temporal completeness) ---\n",
    "if \"year\" in df.columns:\n",
    "    cov_year = df.groupby(\"year\").size().reset_index(name=\"n_records\")\n",
    "    _save(cov_year, os.path.join(RESULTS_DIR, \"qcheck_coverage_year.csv\"))\n",
    "\n",
    "# --- (Optional but useful) anomaly code distribution ---\n",
    "if \"codi_anomalia\" in df.columns:\n",
    "    anom = (\n",
    "        df[\"codi_anomalia\"]\n",
    "        .value_counts(dropna=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"codi_anomalia\", \"codi_anomalia\": \"count\"})\n",
    "    )\n",
    "    _save(anom, os.path.join(RESULTS_DIR, \"qcheck_anomaly_codes.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a52c4",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3.8.2 — Exploratory Analysis on New/Updated Variables\n",
    "We detect **candidate new variables** by excluding a small **baseline** set and then:\n",
    "- Describe numeric columns (`.describe()`)\n",
    "- List top categories for categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238b39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate new columns: ['FECHA_HORA', 'codi_anomalia', 'data_fi', 'data_inici', 'num_serie_contador', 'polissa_id']\n",
      "[saved] C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\results\\iteration_2\\eda_numeric_describe.csv\n",
      "[ok] Wrote EDA summaries to C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\results\\iteration_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_cols = {\n",
    "    \"date\",\"datetime\",\"year\",\n",
    "    \"consumption\",\n",
    "    \"municipi\",\"districte\",\"barri\",\"codi_postal\",\"seccio_censal\",\n",
    "    \"tipus_consum\",\"activitat\"\n",
    "}\n",
    "# polissa_id\tnum_serie_contador\tconsumption\tFECHA_HORA\tcodi_anomalia\tdata_inici\tdata_fi\n",
    "present = set(df.columns)\n",
    "candidate_new = sorted(list(present - baseline_cols))\n",
    "pd.DataFrame({\"candidate_new_columns\": candidate_new}).to_csv(\n",
    "    os.path.join(RESULTS_DIR, \"eda_candidate_new_columns.csv\"), index=False\n",
    ")\n",
    "print(\"Candidate new columns:\", candidate_new)\n",
    "\n",
    "# Numeric describe for all numeric columns (including 'new' ones)\n",
    "num_desc = df.describe(include=\"number\").T\n",
    "_save(num_desc.reset_index().rename(columns={\"index\":\"column\"}),\n",
    "      os.path.join(RESULTS_DIR, \"eda_numeric_describe.csv\"))\n",
    "\n",
    "# Top categories for object category-like columns\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == \"object\" and c not in (\"datetime\",\"date\")]\n",
    "cat_value_counts = {}\n",
    "for c in cat_cols:\n",
    "    vc = df[c].value_counts(dropna=False).head(20)\n",
    "    cat_value_counts[c] = vc.to_dict()\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"eda_top_categories.json\"), \"w\") as f:\n",
    "    json.dump(cat_value_counts, f, indent=2)\n",
    "\n",
    "print(f\"[ok] Wrote EDA summaries to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a3cbc",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3.8.3 — Adjustment of Cleaning Rules if Needed\n",
    "We implement **minimal, reversible** cleaning. The guiding principle is: *do no harm*.\n",
    "- Keep raw values; add **flags** for suspicious records.\n",
    "- Only drop rows if they are clearly invalid (e.g., completely empty or impossible dates).\n",
    "- Save both **cleaned** data and a small **log** of what changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d0868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning changes: {\n",
      "  \"flags_added\": [\n",
      "    \"flag_negative_consumption\",\n",
      "    \"flag_zero_consumption\",\n",
      "    \"flag_anom_32768\",\n",
      "    \"flag_anom_163840\"\n",
      "  ],\n",
      "  \"rows_dropped\": 0\n",
      "}\n",
      "[ok] Saved cleaned data to C:\\Users\\joan\\Desktop\\FEINA\\UPF\\Course\\Fourth_year\\Primer_Trimestre\\Project_Management\\AB\\AB_DataChallenge.Team102D\\results\\iteration_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def minimal_cleaning(df):\n",
    "    df = df.copy()\n",
    "    changes = {\"flags_added\": [], \"rows_dropped\": 0}\n",
    "\n",
    "    # Flag negative and zero consumptions (do not modify the raw value)\n",
    "    if \"consumption\" in df.columns:\n",
    "        df[\"flag_negative_consumption\"] = df[\"consumption\"] < 0\n",
    "        df[\"flag_zero_consumption\"] = df[\"consumption\"] == 0\n",
    "        changes[\"flags_added\"] += [\"flag_negative_consumption\", \"flag_zero_consumption\"]\n",
    "\n",
    "    # Flag known anomaly codes (if present)\n",
    "    if \"codi_anomalia\" in df.columns:\n",
    "        df[\"flag_anom_32768\"] = df[\"codi_anomalia\"] == 32768\n",
    "        df[\"flag_anom_163840\"] = df[\"codi_anomalia\"] == 163840\n",
    "        changes[\"flags_added\"] += [\"flag_anom_32768\", \"flag_anom_163840\"]\n",
    "\n",
    "    # Drop absurd dates (keep most rows)\n",
    "    if \"date\" in df.columns:\n",
    "        # consider years 2000..2030 as valid window\n",
    "        y = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.year\n",
    "        mask_valid_year = y.between(2000, 2030)\n",
    "        before = len(df)\n",
    "        df = df[mask_valid_year].copy()\n",
    "        changes[\"rows_dropped\"] += before - len(df)\n",
    "\n",
    "    # Deduplicate strictly identical rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    changes[\"rows_dropped\"] += before - len(df)\n",
    "\n",
    "    return df, changes\n",
    "\n",
    "clean_df, changes = minimal_cleaning(df)\n",
    "with open(os.path.join(RESULTS_DIR, \"cleaning_changes.json\"), \"w\") as f:\n",
    "    json.dump(changes, f, indent=2)\n",
    "print(\"Cleaning changes:\", json.dumps(changes, indent=2))\n",
    "\n",
    "# Persist cleaned outputs\n",
    "clean_csv = os.path.join(RESULTS_DIR, \"cleaned_dataset_v2.csv\")\n",
    "clean_parquet = os.path.join(RESULTS_DIR, \"cleaned_dataset_v2.parquet\")\n",
    "clean_df.to_csv(clean_csv, index=False)\n",
    "try:\n",
    "    clean_df.to_parquet(clean_parquet, index=False)\n",
    "except Exception as e:\n",
    "    print(f\"[warn] Could not write parquet: {e}\")\n",
    "print(f\"[ok] Saved cleaned data to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f70d1",
   "metadata": {},
   "source": [
    "\n",
    "## Summary — What to Do Next\n",
    "- Review the CSVs in `results/iteration_2/`:\n",
    "  - `qcheck_missingness.csv` and `qcheck_coverage_municipi_year.csv`\n",
    "  - `eda_candidate_new_columns.csv`, `eda_numeric_describe.csv`, `eda_top_categories.json`\n",
    "  - `cleaned_dataset_v2.csv` and `cleaning_changes.json`\n",
    "- If you find issues that require **adjusting rules** (e.g., clamp negatives, impute missing), \n",
    "  add them to `minimal_cleaning()` and **document the rationale** inline.\n",
    "- This notebook finishes **Task 1.3.8** and prepares inputs for **1.3.9 Feature Engineering**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL_QR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
