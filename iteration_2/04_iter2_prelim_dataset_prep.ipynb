{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rweW3WI-YQLJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Notebook section: 1.3.11 Preliminary dataset preparation\n",
        "Team 102D · AB Data Challenge — Iteration 2\n",
        "\n",
        "Inputs\n",
        "  • results/iteration_2/features_v2.csv (or .parquet)\n",
        "  • results/iteration_2/selected_features_v1.txt (from 1.3.10)\n",
        "\n",
        "This script:\n",
        "  1) Loads features and the first‑pass selected feature list\n",
        "  2) Chooses identifier + label columns (y_anom)\n",
        "  3) Train/valid/test split (70/15/15) with Group-aware splitting by meter when available\n",
        "  4) Preprocessing artifacts (median imputer + standard scaler stats)\n",
        "  5) Class balance summary + suggested class weights\n",
        "  6) Saves ready-to-model tables and metadata\n",
        "\n",
        "Outputs (results/iteration_2/prep_1_3_11/)\n",
        "  • split_assignments.csv  — row-wise split label (train/valid/test)\n",
        "  • Xy_train.parquet / Xy_valid.parquet / Xy_test.parquet  — selected features + y\n",
        "  • preprocess_imputer_medians.json, preprocess_scaler_stats.json\n",
        "  • class_balance.json, prep_summary.json, selected_features_resolved.txt\n",
        "  • columns_manifest.json (feature columns, id, label)\n",
        "\n",
        "Notes\n",
        "  • If y is missing everywhere, uses unsupervised random split.\n",
        "  • Keeps id/time/label columns alongside X to ease later joins.\n",
        "\"\"\"\n",
        "\n",
        "# === Imports & setup ===\n",
        "import os, json, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Paths ===\n",
        "PROJECT_ROOT = \".\"\n",
        "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"iteration_2\")\n",
        "PREP_DIR = os.path.join(RESULTS_DIR, \"prep_1_3_11\")\n",
        "os.makedirs(PREP_DIR, exist_ok=True)\n",
        "\n",
        "FEATS_CSV = os.path.join(RESULTS_DIR, \"features_v2.csv\")\n",
        "FEATS_PQ  = os.path.join(RESULTS_DIR, \"features_v2.parquet\")\n",
        "SEL_TXT   = os.path.join(RESULTS_DIR, \"selected_features_v1.txt\")\n",
        "\n",
        "# === Load features ===\n",
        "if os.path.exists(FEATS_CSV):\n",
        "    df = pd.read_csv(FEATS_CSV, low_memory=False)\n",
        "elif os.path.exists(FEATS_PQ):\n",
        "    df = pd.read_parquet(FEATS_PQ)\n",
        "else:\n",
        "    raise SystemExit(\"features_v2 not found under results/iteration_2.\")\n",
        "\n",
        "print(\"Loaded features:\", df.shape)\n",
        "\n",
        "# === Identify columns ===\n",
        "ID_CANDS = [\"num_serie_contador\", \"polissa_id\"]\n",
        "TIME_COLS = [\"datetime\",\"date\",\"data_inici\",\"data_fi\",\"year\",\"month\",\"dayofweek\",\"hour\"]\n",
        "LABEL = \"y_anom\" if \"y_anom\" in df.columns else None\n",
        "\n",
        "id_col = next((c for c in ID_CANDS if c in df.columns), None)\n",
        "\n",
        "# Selected features from 1.3.10\n",
        "if os.path.exists(SEL_TXT):\n",
        "    with open(SEL_TXT, \"r\", encoding=\"utf-8\") as f:\n",
        "        selected = [ln.strip() for ln in f if ln.strip() and ln.strip() in df.columns]\n",
        "else:\n",
        "    # Fallback: take numeric columns not including ids/time/label\n",
        "    exclude = set(ID_CANDS + TIME_COLS + ([LABEL] if LABEL else []) + [\"codi_anomalia\"])\n",
        "    selected = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c not in exclude]\n",
        "\n",
        "# Persist the resolved list\n",
        "with open(os.path.join(PREP_DIR, \"selected_features_resolved.txt\"), \"w\") as f:\n",
        "    for c in selected:\n",
        "        f.write(c + \"\\n\")\n",
        "\n",
        "# === Build the modeling table (X + y + id/time for context) ===\n",
        "keep_cols = list(dict.fromkeys((selected + ([LABEL] if LABEL else []) + ID_CANDS + TIME_COLS)))\n",
        "keep_cols = [c for c in keep_cols if c in df.columns]\n",
        "mod = df[keep_cols].copy()\n",
        "\n",
        "# Ensure numeric dtype on features\n",
        "for c in selected:\n",
        "    mod[c] = pd.to_numeric(mod[c], errors=\"coerce\")\n",
        "\n",
        "# === Split: 70/15/15 ===\n",
        "N = len(mod)\n",
        "split_assign = pd.Series(index=mod.index, dtype=\"string\")\n",
        "\n",
        "has_label = (LABEL is not None) and mod[LABEL].notna().any() and mod[LABEL].nunique() >= 2\n",
        "\n",
        "if id_col is not None:\n",
        "    # Group-aware: split on groups\n",
        "    gss1 = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
        "    tr_idx, hold_idx = next(gss1.split(mod, groups=mod[id_col]))\n",
        "    # valid/test from holdout\n",
        "    hold = mod.iloc[hold_idx]\n",
        "    gss2 = GroupShuffleSplit(n_splits=1, train_size=0.50, random_state=43)\n",
        "    va_idx_rel, te_idx_rel = next(gss2.split(hold, groups=hold[id_col]))\n",
        "    va_idx = hold.index[va_idx_rel]\n",
        "    te_idx = hold.index[te_idx_rel]\n",
        "else:\n",
        "    if has_label:\n",
        "        sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, random_state=42)\n",
        "        tr_idx, hold_idx = next(sss1.split(mod, mod[LABEL].fillna(0)))\n",
        "        sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, random_state=43)\n",
        "        va_rel, te_rel = next(sss2.split(mod.iloc[hold_idx], mod.iloc[hold_idx][LABEL].fillna(0)))\n",
        "        va_idx = mod.iloc[hold_idx].index[va_rel]\n",
        "        te_idx = mod.iloc[hold_idx].index[te_rel]\n",
        "    else:\n",
        "        rng = np.random.default_rng(42)\n",
        "        idx = np.arange(N)\n",
        "        rng.shuffle(idx)\n",
        "        tr_cut = int(0.70*N)\n",
        "        va_cut = tr_cut + int(0.15*N)\n",
        "        tr_idx, va_idx, te_idx = idx[:tr_cut], idx[tr_cut:va_cut], idx[va_cut:]\n",
        "\n",
        "split_assign.loc[tr_idx] = \"train\"\n",
        "split_assign.loc[va_idx] = \"valid\"\n",
        "split_assign.loc[te_idx] = \"test\"\n",
        "\n",
        "# === Preprocessing: imputer medians + (optional) scaler stats ===\n",
        "imp = SimpleImputer(strategy=\"median\")\n",
        "medians = pd.Series(imp.fit(mod[selected]).statistics_, index=selected)\n",
        "\n",
        "# compute scaler stats (mean/std) on TRAIN ONLY to avoid leakage\n",
        "train_means = mod.loc[split_assign == \"train\", selected].mean(numeric_only=True)\n",
        "train_stds  = mod.loc[split_assign == \"train\", selected].std(numeric_only=True).replace(0, np.nan)\n",
        "\n",
        "# Persist preprocess artifacts\n",
        "medians.to_json(os.path.join(PREP_DIR, \"preprocess_imputer_medians.json\"))\n",
        "(pd.DataFrame({\"mean\": train_means, \"std\": train_stds})\n",
        "   .to_json(os.path.join(PREP_DIR, \"preprocess_scaler_stats.json\"), orient=\"table\"))\n",
        "\n",
        "# === Class balance summary ===\n",
        "if has_label:\n",
        "    vc = mod.loc[split_assign == \"train\", LABEL].value_counts(dropna=False).to_dict()\n",
        "    n0 = int(vc.get(0, 0)); n1 = int(vc.get(1, 0))\n",
        "    # Suggested weights (sklearn's 'balanced' formula)\n",
        "    total = n0 + n1 if (n0 + n1) > 0 else 1\n",
        "    w0 = total / (2 * max(n0, 1))\n",
        "    w1 = total / (2 * max(n1, 1))\n",
        "    class_balance = {\"train_counts\": {\"0\": n0, \"1\": n1}, \"suggested_weights\": {\"0\": w0, \"1\": w1}}\n",
        "else:\n",
        "    class_balance = {\"note\": \"Label unavailable or single-class; skipping class balance.\"}\n",
        "\n",
        "with open(os.path.join(PREP_DIR, \"class_balance.json\"), \"w\") as f:\n",
        "    json.dump(class_balance, f, indent=2)\n",
        "\n",
        "# === Write split assignments & datasets ===\n",
        "assign_df = pd.DataFrame({\"row\": mod.index, \"split\": split_assign.values})\n",
        "assign_df.to_csv(os.path.join(PREP_DIR, \"split_assignments.csv\"), index=False)\n",
        "\n",
        "# Save each split with id/time + y + features\n",
        "cols_out = list(dict.fromkeys((ID_CANDS + TIME_COLS + ([LABEL] if LABEL else []) + selected)))\n",
        "cols_out = [c for c in cols_out if c in mod.columns]\n",
        "\n",
        "for name, idx in (\"train\", tr_idx), (\"valid\", va_idx), (\"test\", te_idx):\n",
        "    part = mod.loc[idx, cols_out]\n",
        "    part.to_parquet(os.path.join(PREP_DIR, f\"Xy_{name}.parquet\"), index=False)\n",
        "\n",
        "# === Columns manifest & prep summary ===\n",
        "manifest = {\"features\": selected, \"label\": LABEL, \"id_col\": id_col, \"time_cols\": [c for c in TIME_COLS if c in mod.columns]}\n",
        "with open(os.path.join(PREP_DIR, \"columns_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "summary = {\n",
        "    \"rows\": int(len(mod)),\n",
        "    \"n_features\": int(len(selected)),\n",
        "    \"has_label\": bool(has_label),\n",
        "    \"splits\": assign_df[\"split\"].value_counts(dropna=False).to_dict(),\n",
        "    \"paths\": {\n",
        "        \"train\": \"Xy_train.parquet\",\n",
        "        \"valid\": \"Xy_valid.parquet\",\n",
        "        \"test\":  \"Xy_test.parquet\",\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(PREP_DIR, \"prep_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Saved to:\", PREP_DIR)\n",
        "print(\"Done.\")\n"
      ]
    }
  ]
}