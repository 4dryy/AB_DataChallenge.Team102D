{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11029afa",
   "metadata": {},
   "source": [
    "# 01 – Iteration 3: Data Cleaning for Dataset 2 (Repte Consums Anòmals)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads the **Dataset 2 – \"Consum anomalies facturacio complet_anonymized.parquet\"**.\n",
    "2. Saves a **raw CSV copy** in `data/derived/` for easier inspection and debugging.\n",
    "3. Applies the Iteration 3 basic cleaning rules using `src/cleaning.py`.\n",
    "4. Saves the **cleaned dataset** (CSV + Parquet) and a JSON summary of cleaning actions in:\n",
    "   - `iteration_3/results/cleaned/`\n",
    "\n",
    "The cleaned dataset will be the starting point for:\n",
    "- `02_iter3_feature_engineering.ipynb`\n",
    "- `03_iter3_feature_selection.ipynb`\n",
    "- `04_iter3_dataset_preparation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33edb89",
   "metadata": {},
   "source": [
    "Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure Python can find the src/ package\n",
    "SRC_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.append(SRC_DIR)\n",
    "\n",
    "from cleaning import load_dataset2, apply_basic_cleaning, save_cleaned_outputs\n",
    "\n",
    "# Project root = one level above iteration_3/\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "DERIVED_DIR = os.path.join(DATA_DIR, \"derived\")\n",
    "\n",
    "os.makedirs(DERIVED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR    :\", DATA_DIR)\n",
    "print(\"DERIVED_DIR :\", DERIVED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b641148",
   "metadata": {},
   "source": [
    "Define dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c90c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the new Dataset 2 Parquet file\n",
    "RAW_PARQUET_NAME = \"Consum anomalies facturacio complet_anonymized.parquet\"\n",
    "\n",
    "RAW_PARQUET_PATH = os.path.join(DATA_DIR, RAW_PARQUET_NAME)\n",
    "\n",
    "# CSV copy of the raw dataset for manual inspection\n",
    "RAW_CSV_DERIVED_PATH = os.path.join(\n",
    "    DERIVED_DIR, \"dataset2_raw_iter3_from_parquet.csv\"\n",
    ")\n",
    "\n",
    "# Directory for cleaned outputs inside iteration_3\n",
    "CLEANED_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"results\", \"cleaned\"))\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"RAW_PARQUET_PATH      :\", RAW_PARQUET_PATH)\n",
    "print(\"RAW_CSV_DERIVED_PATH  :\", RAW_CSV_DERIVED_PATH)\n",
    "print(\"CLEANED_DIR           :\", CLEANED_DIR)\n",
    "\n",
    "# Quick existence check\n",
    "if not os.path.exists(RAW_PARQUET_PATH):\n",
    "    raise FileNotFoundError(f\"Raw parquet file not found: {RAW_PARQUET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade2bc8",
   "metadata": {},
   "source": [
    "Load raw parquet & basic inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load raw Dataset 2 from Parquet\n",
    "df_raw = load_dataset2(RAW_PARQUET_PATH)\n",
    "\n",
    "print(\"Raw dataset loaded.\")\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad839fd",
   "metadata": {},
   "source": [
    "Save raw CSV copy in data/derived/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd111503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Save a raw CSV copy in data/derived/ for easier inspection\n",
    "df_raw.to_csv(RAW_CSV_DERIVED_PATH, index=False)\n",
    "print(f\"[ok] Saved raw CSV copy to: {RAW_CSV_DERIVED_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeef110",
   "metadata": {},
   "source": [
    "Apply cleaning rules (using cleaning.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6734aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Apply Iteration 3 basic cleaning rules\n",
    "df_clean, cleaning_changes = apply_basic_cleaning(df_raw)\n",
    "\n",
    "print(\"Cleaned dataset created.\")\n",
    "print(\"Cleaned shape:\", df_clean.shape)\n",
    "\n",
    "# Quick sanity checks\n",
    "display(df_clean.head())\n",
    "print(\"\\nCleaning summary (first-level keys):\")\n",
    "for k in cleaning_changes.keys():\n",
    "    print(f\"  - {k}: {cleaning_changes[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2b037",
   "metadata": {},
   "source": [
    "Save cleaned dataset + JSON summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9017025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Persist cleaned dataset and cleaning summary\n",
    "BASE_NAME = \"dataset2_cleaned_iter3\"\n",
    "\n",
    "save_cleaned_outputs(\n",
    "    clean_df=df_clean,\n",
    "    changes=cleaning_changes,\n",
    "    out_dir=CLEANED_DIR,\n",
    "    base_name=BASE_NAME,\n",
    ")\n",
    "\n",
    "# For convenience, show final paths\n",
    "clean_csv_path = os.path.join(CLEANED_DIR, f\"{BASE_NAME}.csv\")\n",
    "clean_parquet_path = os.path.join(CLEANED_DIR, f\"{BASE_NAME}.parquet\")\n",
    "clean_json_path = os.path.join(CLEANED_DIR, f\"{BASE_NAME}_cleaning_changes.json\")\n",
    "\n",
    "print(\"\\nFinal outputs:\")\n",
    "print(\"  Clean CSV   :\", clean_csv_path)\n",
    "print(\"  Clean Parquet:\", clean_parquet_path)\n",
    "print(\"  Summary JSON :\", clean_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfee81",
   "metadata": {},
   "source": [
    "## Quick descriptive stats (for documentation)\n",
    "\n",
    "The cell below computes a few basic statistics useful for the Iteration 3 report:\n",
    "- number of rows / columns after cleaning\n",
    "- anomaly label distribution (if available)\n",
    "- negative / zero consumption counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e221744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaned dataset shape:\", df_clean.shape)\n",
    "\n",
    "# Try to find anomaly-related columns\n",
    "anom_cols = [c for c in df_clean.columns if \"anom\" in c.lower()]\n",
    "print(\"Anomaly-related columns:\", anom_cols)\n",
    "\n",
    "if anom_cols:\n",
    "    print(\"\\nValue counts for first anomaly column:\")\n",
    "    display(df_clean[anom_cols[0]].value_counts(dropna=False).head(20))\n",
    "\n",
    "for col in [\"flag_negative_consumption\", \"flag_zero_consumption\",\n",
    "            \"flag_anom_32768\", \"flag_anom_163840\"]:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"{col}: {df_clean[col].sum()} rows True\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
